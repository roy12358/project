import requests
from bs4 import BeautifulSoup
import json
import time
import random
import re
import os
from concurrent.futures import ThreadPoolExecutor
from urllib.parse import urljoin, unquote
import sys
import io
from functools import partial

# ==============================================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ åªéœ€è¦ä¿®æ”¹é€™è£¡ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ==============================================================================
# 1. è¨­å®šè¦çˆ¬å–çš„ç›®æ¨™
#    - TARGET_AREA: è«‹å¾ä¸‹æ–¹åŸ·è¡Œå¾Œé¡¯ç¤ºçš„ã€Œå¯é¸å€åŸŸåˆ—è¡¨ã€ä¸­ï¼Œå®Œæ•´è¤‡è£½ä¸€å€‹åç¨±è²¼ä¸Šã€‚
#    - TARGET_KANA_ROW: å¡«å¯« 'all' æˆ–å‡åè¡Œçš„ä»£è¡¨å­—æ¯ (å¦‚ 'a', 'k', 's', 't' ç­‰)ã€‚
TARGET_CITY = "tokyo"
TARGET_AREA = "ä¹æ®µä¸‹"
TARGET_KANA_ROW = "k"  # <-- è¼¸å…¥ 'k' æœƒæŠ“å– ka, ki, ku, ke, ko æ‰€æœ‰é é¢

# 2. (å¯é¸) èª¿æ•´çˆ¬èŸ²æ•ˆèƒ½èˆ‡è¡Œç‚º
MAX_WORKERS = 8
DELAY_RANGE = (0.5, 1.5)
TIMEOUT = 15
# ==============================================================================
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ä»¥ä¸Šæ˜¯å”¯ä¸€éœ€è¦ä¿®æ”¹çš„å€åŸŸ â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
# ==============================================================================


# --- å‡åè¡Œå°ç…§è¡¨ (ç„¡éœ€ä¿®æ”¹) ---
KANA_ROW_MAP = {
    'a': ['a', 'i', 'u', 'e', 'o'],
    'k': ['ka', 'ki', 'ku', 'ke', 'ko'],
    's': ['sa', 'shi', 'su', 'se', 'so'],
    't': ['ta', 'chi', 'tsu', 'te', 'to'],
    'n': ['na', 'ni', 'nu', 'ne', 'no'],
    'h': ['ha', 'hi', 'fu', 'he', 'ho'],
    'm': ['ma', 'mi', 'mu', 'me', 'mo'],
    'y': ['ya', 'yu', 'yo'],
    'r': ['ra', 'ri', 'ru', 're', 'ro'],
    'w': ['wa', 'wo'],
    # æ¿éŸ³/åŠæ¿éŸ³è¡Œ
    'g': ['ga', 'gi', 'gu', 'ge', 'go'],
    'z': ['za', 'ji', 'zu', 'ze', 'zo'],
    'd': ['da', 'ji', 'zu', 'de', 'do'], # é€™è£¡çš„ ji, zu æ˜¯å¸¸è¦‹æ›¿ä»£
    'b': ['ba', 'bi', 'bu', 'be', 'bo'],
    'p': ['pa', 'pi', 'pu', 'pe', 'po'],
    'v': ['v'] # ç‰¹æ®Šè¡Œ
}

# --- å°ˆæ¡ˆè·¯å¾‘èˆ‡å…¨åŸŸè¨­å®š (ç„¡éœ€ä¿®æ”¹) ---
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RAW_JSON_DIR = os.path.join(BASE_DIR, "data", "raw")
PHOTO_DIR = os.path.join(BASE_DIR, "data", "photos")

session = requests.Session()
session.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
})

# --- æ ¸å¿ƒçˆ¬èŸ²å‡½å¼ (ç„¡éœ€ä¿®æ”¹) ---

def fetch_url(url):
    try:
        time.sleep(random.uniform(*DELAY_RANGE))
        response = session.get(url, timeout=TIMEOUT, headers={"Referer": f"https://tabelog.com/sitemap/{TARGET_CITY}/"})
        response.raise_for_status()
        return response
    except requests.exceptions.RequestException as e:
        print(f"  - è«‹æ±‚éŒ¯èª¤ {url}: {e}", file=sys.stderr)
        return None

def get_named_links_from_page(page_url, selector):
    res = fetch_url(page_url)
    if not res: return {}
    soup = BeautifulSoup(res.text, "html.parser")
    # å°‡å€åŸŸåç¨±ä¸­çš„åŠå½¢ç©ºæ ¼æ›¿æ›ç‚ºå…¨å½¢ç©ºæ ¼ï¼Œä»¥åŒ¹é…ç¶²ç«™ä¸Šçš„é¡¯ç¤º
    return {a.get_text(strip=True).replace(" ", "ã€€"): urljoin(page_url, a["href"]) for a in soup.select(selector) if a.get("href")}

def get_available_kana_paths(area_url):
    """å¾å€åŸŸé é¢ç²å–æ‰€æœ‰å¯ç”¨çš„å‡å URL è·¯å¾‘ (e.g., ['a', 'i', 'ka', 'ki'])"""
    res = fetch_url(area_url)
    if not res: return []
    soup = BeautifulSoup(res.text, "html.parser")
    paths = []
    for a_tag in soup.select("ul.sitemap-50on__list li a"):
        href = a_tag.get("href", "").strip('/')
        if href:
            path_part = os.path.basename(href)
            if path_part:
                paths.append(path_part)
    return paths

def get_photo_urls(restaurant_url):
    photo_urls = {"exterior": [], "interior": []}
    photo_category_pages = {"interior": urljoin(restaurant_url, "dtlphotolst/3/smp2/"), "exterior": urljoin(restaurant_url, "dtlphotolst/4/smp2/")}
    for category, page_url in photo_category_pages.items():
        current_url = page_url
        while current_url:
            res = fetch_url(current_url)
            if not res or res.status_code == 404: break
            soup = BeautifulSoup(res.text, "html.parser")
            img_tags = soup.select('div.rstdtl-thumb-list__img-wrap img')
            for img in img_tags:
                img_url = img.get("src")
                if img_url:
                    high_res_url = re.sub(r'/150x150_square_/', '/1200x1200_square_/', img_url)
                    photo_urls[category].append(high_res_url)
            next_page_tag = soup.select_one("a.c-pagination__arrow--next")
            current_url = urljoin(current_url, next_page_tag["href"]) if next_page_tag else None
    return photo_urls

def get_restaurant_basic(url):
    res = fetch_url(url)
    if not res: return None
    soup = BeautifulSoup(res.text, "html.parser")
    rating_tag = soup.select_one('span.rdheader-rating__score-val-dtl')
    genre_tag = soup.select_one("th:-soup-contains('ã‚¸ãƒ£ãƒ³ãƒ«') + td")
    address_tag = soup.select_one('p.rstinfo-table__address')
    hours_tag = soup.select_one("th:-soup-contains('å–¶æ¥­æ™‚é–“') + td")
    
    # ===== ä¿®æ­£æœ€è¿‘è»Šç«™çš„çˆ¬å–é‚è¼¯ =====
    station_info = None
    
    # æ–¹æ³•1: å¾headerå€åŸŸçš„æœ€å¯„ã‚Šé§…ç²å–
    station_dt = soup.find('dt', string=lambda text: text and 'æœ€å¯„ã‚Šé§…' in text)
    if station_dt:
        station_dd = station_dt.find_next_sibling('dd')
        if station_dd:
            station_span = station_dd.select_one('span.linktree__parent-target-text')
            if station_span:
                station_info = station_span.get_text(strip=True)
    
    # å‚™é¸æ–¹æ³•: å¦‚æœä¸Šé¢æ²’æ‰¾åˆ°ï¼Œå˜—è©¦è¡¨æ ¼ä¸­çš„äº¤é€šæ‰‹æ®µ
    if not station_info:
        station_tag_alt = soup.select_one("th:-soup-contains('äº¤é€šæ‰‹æ®µ') + td")
        if station_tag_alt:
            station_text = station_tag_alt.get_text(strip=True)
            station_match = re.search(r'([^ã€\n]*é§…)', station_text)
            if station_match:
                station_info = station_match.group(1).strip()
    # =======================================
    
    budget_dinner_tag = soup.select_one(".rstinfo-table__budget-item:has(.c-rating-v3__time--dinner) em")
    budget_lunch_tag = soup.select_one(".rstinfo-table__budget-item:has(.c-rating-v3__time--lunch) em")
    budget = {}
    if budget_dinner_tag: budget['dinner'] = budget_dinner_tag.get_text(strip=True)
    if budget_lunch_tag: budget['lunch'] = budget_lunch_tag.get_text(strip=True)
    
    return {
        "rating": rating_tag.get_text(strip=True) if rating_tag else None,
        "genre": genre_tag.get_text(strip=True).replace("\n", " ") if genre_tag else None,
        "address": address_tag.get_text(strip=True).replace("å¤§ããªåœ°å›³ã‚’è¦‹ã‚‹", "").strip() if address_tag else None,
        "station_info": station_info,  # ä½¿ç”¨ä¿®æ­£å¾Œçš„è»Šç«™è³‡è¨Š
        "hours": hours_tag.get_text(strip=True) if hours_tag else None,
        "budget": budget if budget else None,
    }

def get_all_review_urls(restaurant_url):
    review_list_url = urljoin(restaurant_url, "dtlrvwlst/")
    all_review_urls = []
    current_url = review_list_url
    while current_url:
        res = fetch_url(current_url)
        if not res: break
        soup = BeautifulSoup(res.text, "html.parser")
        review_links = soup.select("a.js-link-bookmark-detail[data-detail-url]")
        if not review_links: break
        for link in review_links:
            detail_url_path = link.get('data-detail-url')
            if detail_url_path:
                all_review_urls.append(urljoin("https://tabelog.com", detail_url_path))
        next_page_tag = soup.select_one("a.c-pagination__arrow--next")
        current_url = urljoin(current_url, next_page_tag["href"]) if next_page_tag else None
    return all_review_urls

def get_review_detail(review_url):
    res = fetch_url(review_url)
    if not res: return None
    soup = BeautifulSoup(res.text, "html.parser")
    title_tag = soup.select_one("p.rvw-item__title strong")
    comment_tag = soup.select_one("div.rvw-item__rvw-comment p")
    title_text = title_tag.get_text(strip=True) if title_tag else None
    comment_text = comment_tag.get_text(strip=True) if comment_tag else None
    if not title_text and not comment_text:
        return None
    user_rating_tag = soup.select_one("b.rvw-item__ratings--val")
    detail_ratings = {}
    for li in soup.select("ul.c-rating-detail li"):
        spans = li.find_all(recursive=False)
        if len(spans) == 2:
            label_text = spans[0].get_text(strip=True)
            score_text = spans[1].get_text(strip=True)
            detail_ratings[label_text] = score_text
        elif len(spans) == 1 and li.strong:
            label_text = li.strong.previous_sibling.strip()
            score_text = li.strong.get_text(strip=True)
            if label_text:
                 detail_ratings[label_text] = score_text
    return {
        "title": title_text,
        "comment": comment_text,
        "user_rating": user_rating_tag.get_text(strip=True) if user_rating_tag else None,
        "detail_ratings": detail_ratings if detail_ratings else None,
        "url": review_url
    }

def download_photo(url, filepath):
    if os.path.exists(filepath): return True
    try:
        res = session.get(url, stream=True, timeout=TIMEOUT)
        res.raise_for_status()
        with open(filepath, 'wb') as f: f.write(res.content)
        return True
    except requests.exceptions.RequestException: return False

def process_restaurant(restaurant_data, photo_dir):
    name, url = restaurant_data
    try:
        tabelog_id = re.search(r'/(\d+)/?$', url).group(1)
    except (AttributeError, IndexError): return None
    print(f"ğŸ”„ è™•ç†ä¸­: {name} (ID: {tabelog_id})")
    detail = get_restaurant_basic(url)
    if not detail:
        print(f"  âŒ ç„¡æ³•å–å¾—åŸºæœ¬è³‡æ–™: {name}", file=sys.stderr)
        return None
    
    review_urls = get_all_review_urls(url)
    photo_urls = get_photo_urls(url)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        review_futures = [executor.submit(get_review_detail, r_url) for r_url in review_urls]
        photo_futures, local_photo_filenames = [], {k: [] for k in photo_urls}
        for category, urls in photo_urls.items():
            for i, p_url in enumerate(urls):
                ext = os.path.splitext(unquote(p_url.split('?')[0]))[1] or '.jpg'
                filename = f"{tabelog_id}_{category}_{i+1:02d}{ext}"
                filepath = os.path.join(photo_dir, filename)
                photo_futures.append(executor.submit(download_photo, p_url, filepath))
                local_photo_filenames[category].append(filename)
        reviews = [f.result() for f in review_futures if f.result()]
        [f.result() for f in photo_futures]
    result = {"tabelog_id": tabelog_id, "name": name, "url": url, **detail, "photos": local_photo_filenames, "reviews": reviews}
    print(f"âœ… å®Œæˆ: {name} (å·²æŠ“å–è©•è«–: {len(reviews)}, ç…§ç‰‡: {sum(len(v) for v in photo_urls.values())})")
    return result

def scrape_restaurant_list_page(page_url, photo_dir):
    restaurants = []
    current_url = page_url
    process_func = partial(process_restaurant, photo_dir=photo_dir)
    while current_url:
        print(f"\n--- æ­£åœ¨çˆ¬å–åˆ—è¡¨é é¢: {current_url} ---")
        res = fetch_url(current_url)
        if not res: break
        soup = BeautifulSoup(res.text, "html.parser")
        restaurant_links = soup.select("a.sitemap-50dtl__name")
        if not restaurant_links: break
        restaurant_data = [(a.get_text(strip=True), urljoin(current_url, a["href"])) for a in restaurant_links]
        with ThreadPoolExecutor(max_workers=4) as executor:
            results = list(executor.map(process_func, restaurant_data))
            restaurants.extend([r for r in results if r])
        next_page_tag = soup.select_one("a.c-pagination__arrow--next")
        current_url = urljoin(current_url, next_page_tag["href"]) if next_page_tag else None
    return restaurants

def run_pre_flight_check():
    print("--------------------------------------------------")
    print("ğŸš€ Tabelog çˆ¬èŸ²å•Ÿå‹•å‰æª¢æŸ¥...")
    city_url = f"https://tabelog.com/sitemap/{TARGET_CITY}/"
    area_links = get_named_links_from_page(city_url, "ul.area-content__list a.area-content__item-target")
    if not area_links:
        print(f"âŒ éŒ¯èª¤ï¼šç„¡æ³•å¾ {city_url} ç²å–ä»»ä½•ä¸»è¦å€åŸŸåˆ—è¡¨ã€‚")
        sys.exit(1)
    
    sub_area_links = get_named_links_from_page(city_url, "ul.area-list__item-sub a")
    area_links.update(sub_area_links)
    
    print(f"\nğŸ™ï¸ åœ¨ {TARGET_CITY.upper()} ä¸­æ‰¾åˆ°ä»¥ä¸‹å¯é¸å€åŸŸåˆ—è¡¨ï¼š")
    for name in sorted(area_links.keys()): print(f"  - {name}")
    if TARGET_AREA not in area_links:
        print("\n" + "="*50)
        print(f"âŒ è¨­å®šéŒ¯èª¤ï¼šæ‚¨è¨­å®šçš„ç›®æ¨™å€åŸŸ '{TARGET_AREA}' ä¸åœ¨å¯é¸åˆ—è¡¨ä¸­ã€‚")
        print("ğŸ‘‰ è«‹å¾ä¸Šé¢çš„åˆ—è¡¨ä¸­ï¼Œå®Œæ•´è¤‡è£½ä¸€å€‹æœ‰æ•ˆçš„å€åŸŸåç¨±åˆ°æª”æ¡ˆé ‚éƒ¨çš„ TARGET_AREAã€‚")
        print("="*50)
        sys.exit(1)
    print("\nâœ… å€åŸŸè¨­å®šæ­£ç¢ºï¼")
    return area_links[TARGET_AREA]

def main():
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    target_area_url = run_pre_flight_check()

    print("\n--------------------------------------------------")
    print(f"ğŸ¯ æº–å‚™çˆ¬å–ç›®æ¨™:")
    print(f"   - åŸå¸‚: {TARGET_CITY}")
    print(f"   - å€åŸŸ: {TARGET_AREA}")
    print(f"   - å‡åè¡Œ: {TARGET_KANA_ROW}")
    print("--------------------------------------------------\n")
    time.sleep(2)

    os.makedirs(RAW_JSON_DIR, exist_ok=True)
    os.makedirs(PHOTO_DIR, exist_ok=True)
    
    urls_to_scrape = []
    input_kana_row = TARGET_KANA_ROW.lower()

    # <<<--- å…¨æ–°ã€ç¬¦åˆæ‚¨æœŸæœ›çš„ URL ç”Ÿæˆé‚è¼¯ ---<<<
    available_paths = get_available_kana_paths(target_area_url)
    if not available_paths:
        print(f"âŒ éŒ¯èª¤: ç„¡æ³•å¾ '{TARGET_AREA}' é é¢ç²å–ä»»ä½•å¯ç”¨çš„å‡åè¡Œé€£çµã€‚")
        return

    if input_kana_row == 'all':
        print(f"ğŸ” æ¨¡å¼ 'all': æº–å‚™çˆ¬å–æ‰€æœ‰ {len(available_paths)} å€‹å¯ç”¨çš„å‡åè¡Œã€‚")
        urls_to_scrape = [urljoin(target_area_url, f"{path}/") for path in available_paths]
    
    elif input_kana_row in KANA_ROW_MAP:
        # å¾å°ç…§è¡¨ç²å–è©²è¡Œçš„æ‰€æœ‰ç¾…é¦¬æ‹¼éŸ³
        romaji_in_row = KANA_ROW_MAP[input_kana_row]
        
        # ç¯©é¸å‡ºåœ¨è©²é é¢å¯¦éš›å­˜åœ¨çš„è·¯å¾‘
        paths_to_scrape = [r for r in romaji_in_row if r in available_paths]
        
        if not paths_to_scrape:
            print(f"âŒ è³‡è¨Š: åœ¨ '{TARGET_AREA}' å€åŸŸä¸­ï¼Œæ²’æœ‰æ‰¾åˆ°ä»»ä½•å±¬æ–¼ '{input_kana_row}' è¡Œçš„é¤å»³åˆ†é ã€‚")
            print(f"   (å¯ç”¨çš„åˆ†é æœ‰: {', '.join(available_paths)})")
            return
            
        print(f"âœ… æ‰¾åˆ° '{input_kana_row}' è¡Œä¸­å¯ç”¨çš„åˆ†é : {', '.join(paths_to_scrape)}")
        urls_to_scrape = [urljoin(target_area_url, f"{path}/") for path in paths_to_scrape]

    else:
        print(f"âŒ è¨­å®šéŒ¯èª¤: TARGET_KANA_ROW '{TARGET_KANA_ROW}' ä¸æ˜¯ 'all' æˆ–ä¸€å€‹æœ‰æ•ˆçš„è¡Œä»£è¡¨å­—æ¯ã€‚")
        print(f"ğŸ‘‰ å¯ç”¨çš„è¡Œä»£è¡¨å­—æ¯æœ‰: {', '.join(KANA_ROW_MAP.keys())}")
        return

    if not urls_to_scrape:
        print("â„¹ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•éœ€è¦çˆ¬å–çš„ URLï¼Œç¨‹å¼çµæŸã€‚")
        return
    # <<<--------------------------------------------------<<<

    all_restaurants = []
    for kana_url in urls_to_scrape:
        restaurants = scrape_restaurant_list_page(kana_url, PHOTO_DIR)
        all_restaurants.extend(restaurants)
        if restaurants:
            safe_area = re.sub(r'[\\/:*?"<>|]', '-', TARGET_AREA)
            row_code = os.path.basename(kana_url.strip('/'))
            filename = f"{TARGET_CITY}_{safe_area}_{row_code}.json"
            filepath = os.path.join(RAW_JSON_DIR, filename)
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(restaurants, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ’¾ æ•¸æ“šå·²å„²å­˜è‡³: {filepath}")

    print("\n\n" + "="*50)
    print(f"ğŸ å…¨éƒ¨å®Œæˆï¼æœ¬æ¬¡å…±çˆ¬å–äº† {len(all_restaurants)} é–“é¤å»³ã€‚")
    print(f"   - JSON æª”æ¡ˆä½æ–¼: {RAW_JSON_DIR}")
    print(f"   - ç…§ç‰‡æª”æ¡ˆä½æ–¼: {PHOTO_DIR}")
    print("="*50)

if __name__ == "__main__":
    main()